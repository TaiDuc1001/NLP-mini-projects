{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install WandB"
      ],
      "metadata": {
        "id": "AfhQo2lSf4Zz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uZoC1GEaC6Hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f52eec-2d21-4411-9735-9cab69c21571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "_LZ9-ZmZgBfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "\n",
        "import pandas as pd\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "I8DAIPJxDD3j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract"
      ],
      "metadata": {
        "id": "kYgX0WezgIPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_ZIP_PATH = \"/content/drive/MyDrive/Datasets/Toxic Comments/glove.6B.zip\"\n",
        "ZIP_PATH = \"/content/drive/MyDrive/Datasets/Toxic Comments/jigsaw-toxic-comment-classification-challenge.zip\"\n",
        "if not os.path.exists(\"/content/data\"):\n",
        "    os.mkdir(\"/content/data\")\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as zipref:\n",
        "    zipref.extractall(\"/content/data\")\n",
        "SUB_ZIP_PATHS = os.listdir(\"/content/data\")\n",
        "for zip_file in SUB_ZIP_PATHS:\n",
        "    full_path = os.path.join(\"/content/data\", zip_file)\n",
        "    with zipfile.ZipFile(full_path, \"r\") as zipref:\n",
        "        zipref.extractall(\"/content/data\")\n",
        "\n",
        "# Extract GLOVE\n",
        "if not os.path.exists(\"/content/glove\"):\n",
        "    os.mkdir(\"/content/glove\")\n",
        "with zipfile.ZipFile(GLOVE_ZIP_PATH, \"r\") as zipref:\n",
        "    zipref.extractall(\"/content/glove\")"
      ],
      "metadata": {
        "id": "bJymoTV9D4tF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"ARCHITECTURE\": \"CNN\",\n",
        "    \"DATASET\": \"jigsaw_toxic_comments\",\n",
        "    \"WEIGHTS\": \"STANFORD GLOVE.6B\",\n",
        "    \"MAX_SEQUENCE_LENGTH\": 100,\n",
        "    \"MAX_VOCAB_SIZE\": 20000,\n",
        "    \"EMBED_SIZE\": 100,\n",
        "    \"VALIDATION_SPLIT\": 0.2,\n",
        "    \"BATCH_SIZE\": 128,\n",
        "    \"EPOCHS\": 10\n",
        "}\n",
        "wandb.init(project=\"Toxic_Comment_with_CNN\", config=config)\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "BeueKWR2FOE-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7cc24493-9d0c-452f-ace0-2e25bba942b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaiduc1001\u001b[0m (\u001b[33mduckyhome\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240411_015312-ao14pppe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/duckyhome/Toxic_Comment_with_CNN/runs/ao14pppe' target=\"_blank\">faithful-mountain-3</a></strong> to <a href='https://wandb.ai/duckyhome/Toxic_Comment_with_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/duckyhome/Toxic_Comment_with_CNN' target=\"_blank\">https://wandb.ai/duckyhome/Toxic_Comment_with_CNN</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/duckyhome/Toxic_Comment_with_CNN/runs/ao14pppe' target=\"_blank\">https://wandb.ai/duckyhome/Toxic_Comment_with_CNN/runs/ao14pppe</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Load word vectors...\")\n",
        "word2vec = {}\n",
        "with open(f\"/content/glove/glove.6B.{config.EMBED_SIZE}d.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.array(values[1:], dtype=\"float32\")\n",
        "        word2vec[word] = vec\n",
        "print(f\"Found {len(word2vec)} word vectors with length {len(vec)}.\")"
      ],
      "metadata": {
        "id": "pzydQ23FFkDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839c16a9-1d02-49b9-99aa-9cbfcd3886fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load word vectors...\n",
            "Found 400000 word vectors with length 100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Load comments...\")\n",
        "train = pd.read_csv(\"/content/data/train.csv\")\n",
        "sentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\n",
        "possible_labels = train.columns.values[2:]\n",
        "targets = train[possible_labels].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Fd8TR9hrco",
        "outputId": "046f5ca7-b68d-4607-a2e8-ad34c56c0f4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load comments...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=config.MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "id": "Lw_j_O6qiyF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Max sequence length: {max(len(s) for s in sequences)}\")\n",
        "print(f\"Min sequence length: {min(len(s) for s in sequences)}\")\n",
        "s = sorted(len(s) for s in sequences)\n",
        "print(f\"Median sequence length: {s[len(s)//2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnL8VVnHj_Ya",
        "outputId": "ecb0da9e-8dc0-4cd8-f9b3-fd9ffd9f1d35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 1400\n",
            "Min sequence length: 0\n",
            "Median sequence length: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tokenizer.word_index\n",
        "print(f\"Found {len(word2idx)} unique tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_35Gi9EVkYlH",
        "outputId": "db81e058-14c7-448c-d474-bb84db20d8db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 210337 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences, maxlen=config.MAX_SEQUENCE_LENGTH)\n",
        "print(f\"Shape of data tensor: \", data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAhIKszQlskl",
        "outputId": "c180c362-4466-4c18-c714-3049d363cae7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor:  (159571, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 159571 là số training example\n",
        "# 1400 là độ dài lớn nhất của example\n",
        "# 210337 là số unique token, hoặc là số unique word\n",
        "# (159671, 100) là shape của bộ dataset khi đã tokenize, rows là số example, columns là số word tối đa sau khi bị crop và pad"
      ],
      "metadata": {
        "id": "PrpyZyNwnkmr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Filling pre-trained embeddings...\")\n",
        "num_words = min(config.MAX_VOCAB_SIZE, len(word2idx)+1)\n",
        "embedding_matrix = np.zeros((num_words, config.EMBED_SIZE))\n",
        "for word, i in word2idx.items():\n",
        "    if i < config.MAX_VOCAB_SIZE:\n",
        "        embedding_vector = word2vec.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXvecIXxoYwi",
        "outputId": "3352191c-44df-45ae-978c-0b8255b7d5bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filling pre-trained embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNToxicComment(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_layer = Embedding(\n",
        "            input_dim = num_words,\n",
        "            output_dim = config.EMBED_SIZE,\n",
        "            weights = [embedding_matrix],\n",
        "            input_length = config.MAX_SEQUENCE_LENGTH,\n",
        "            trainable = False\n",
        "        )\n",
        "        self.body = keras.Sequential([\n",
        "            Conv1D(128, 3, activation='relu'),\n",
        "            MaxPooling1D(3),\n",
        "            Conv1D(128, 3, activation='relu'),\n",
        "            MaxPooling1D(3),\n",
        "            Conv1D(128, 3, activation='relu'),\n",
        "            GlobalMaxPooling1D(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(6, activation='sigmoid')\n",
        "        ])\n",
        "    def call(self, x):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.body(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eUJCVvFoqvbx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    WandbCallback(\n",
        "        monitor='val_loss',\n",
        "        mode='auto'\n",
        "    ),\n",
        "    WandbCallback(\n",
        "        monitor='val_accuracy',\n",
        "        mode='auto'\n",
        "    ),\n",
        "    WandbCallback(\n",
        "        monitor='loss',\n",
        "        mode='auto'\n",
        "    ),\n",
        "    WandbCallback(\n",
        "        monitor='accuracy',\n",
        "        mode='auto'\n",
        "    )\n",
        "]\n",
        "\n",
        "model = CNNToxicComment()\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='rmsprop',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.fit(\n",
        "    data,\n",
        "    targets,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    epochs=config.EPOCHS,\n",
        "    validation_split=config.VALIDATION_SPLIT,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "nSfuKu7kzCqt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}